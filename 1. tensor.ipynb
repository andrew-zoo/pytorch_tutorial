{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0521, 0.7710, 0.8648],\n",
      "        [0.2096, 0.8417, 0.2557],\n",
      "        [0.1497, 0.9253, 0.6093],\n",
      "        [0.2775, 0.3009, 0.9422],\n",
      "        [0.9579, 0.0613, 0.8080]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    tensor=tensor.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row: tensor([1., 1., 1., 1.])\n",
      "First column: tensor([1., 1., 1., 1.])\n",
      "Second column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4,4)\n",
    "print(\"First row:\",tensor[0])\n",
    "print(\"First column:\",tensor[:,0])\n",
    "print(\"Second column:\",tensor[:,1])\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.ones(4,4)\n",
    "tensor2 = torch.zeros(4,4)\n",
    "\n",
    "t1 = torch.cat([tensor1,tensor2], dim=1)        #텐서 합치기 -> troch.cat\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 4., 4., 4.],\n",
       "        [4., 4., 4., 4.],\n",
       "        [4., 4., 4., 4.],\n",
       "        [4., 4., 4., 4.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "\n",
    "y3 = torch.rand_like(y1)\n",
    "torch.matmul(tensor, tensor.T, out=y3)  # 결과를 y3에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.)\n",
      "tensor(12.) <class 'torch.Tensor'>\n",
      "12.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "agg = tensor.sum()  # 텐서의 모든 원소를 합산\n",
    "agg = tensor.sum(0)  # 텐서의 모든 원소를 합산  ->dim 0 방향으로 합 # (행 방향)\n",
    "agg = tensor.sum(1)  # 텐서의 모든 원소를 합산  ->dim 1 방향으로 합 # (열 방향)\n",
    "# agg = tensor.sum(2)  # 텐서의 모든 원소를 합산  # ->dim 2 방향으로 합 # (3차원 텐서에서 사용)agg = tensor.sum()  # 텐서의 모든 원소를 합산\n",
    "agg = tensor.sum()  # 텐서의 모든 원소를 합산\n",
    "print(agg)\n",
    "agg_item = agg.item()\n",
    "print(agg, type(agg))\n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[16., 15., 16., 16.],\n",
      "        [16., 15., 16., 16.],\n",
      "        [16., 15., 16., 16.],\n",
      "        [16., 15., 16., 16.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[21., 20., 21., 21.],\n",
       "        [21., 20., 21., 21.],\n",
       "        [21., 20., 21., 21.],\n",
       "        [21., 20., 21., 21.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tensor)\n",
    "tensor.add_(5)  #tensor.add(5)랑 무슨 차이가 있을까? add()는 메모리에 새로운 변수를 할당하지만,\n",
    "                #add_()는 기존 메모리를 재사용해서 값을 변경함. 이를 in-place operation이라고 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:  tensor([1., 1., 1., 1., 1.])\n",
      "n:  [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "n = t.numpy()   # 텐서를 넘파이 배열로 변환  bridge 라고 부름\n",
    "print(\"t: \", t)\n",
    "print(\"n: \",n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t:  tensor([2., 2., 2., 2., 2.])\n",
      "n:  [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t.add_(1)   # 텐서에 1을 더함 (in-place operation), 메모리 재사용\n",
    "print(\"t: \",t)\n",
    "print(\"n: \",n)  # 넘파이 배열도 변경됨, 텐서와 넘파이 배열은 메모리를 공유함 -> 매우 중요"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
